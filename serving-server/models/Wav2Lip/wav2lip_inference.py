"""
Wav2Lip Inference Module - ëª¨ë¸ ì¬ì‚¬ìš©ì„ ìœ„í•œ í•¨ìˆ˜í˜• ì¸í„°í˜ì´ìŠ¤
"""
import numpy as np
import cv2
import os
import subprocess
import platform
import torch
import face_detection
import audio
from models import Wav2Lip
from tqdm import tqdm
import time

mel_step_size = 16

def increase_frames(frames, target_length):
	"""
	ì˜ìƒ í”„ë ˆì„ì„ ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë§ì¶° ê· ë“±í•˜ê²Œ í™•ì¥ (ë²¡í„°í™” ìµœì í™”)
	í”„ë ˆì„ì„ ë³µì œí•˜ì—¬ ëª©í‘œ ê¸¸ì´ê¹Œì§€ ëŠ˜ë¦¼ (ìˆœí™˜ ì¬ìƒì´ ì•„ë‹Œ ì—°ì† í™•ì¥)
	
	Args:
		frames: ì›ë³¸ í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸
		target_length: ëª©í‘œ í”„ë ˆì„ ìˆ˜
		
	Returns:
		í™•ì¥ëœ í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸
	"""
	if len(frames) >= target_length:
		return frames[:target_length]
	
	# ë²¡í„°í™”ëœ í™•ì¥: ê° í”„ë ˆì„ì˜ ë°˜ë³µ íšŸìˆ˜ë¥¼ ë¯¸ë¦¬ ê³„ì‚°
	original_len = len(frames)
	ratio = target_length / original_len
	
	# ê° í”„ë ˆì„ì´ ëª‡ ë²ˆ ë°˜ë³µë˜ì–´ì•¼ í•˜ëŠ”ì§€ ê³„ì‚°
	repeat_counts = np.ones(original_len, dtype=int)
	remaining = target_length - original_len
	
	# ê· ë“±í•˜ê²Œ ë¶„ë°°
	if remaining > 0:
		dup_every = original_len / remaining
		next_dup = 0.0
		added = 0
		
		while added < remaining and next_dup < original_len:
			idx = int(next_dup)
			if idx < original_len:
				repeat_counts[idx] += 1
				added += 1
			next_dup += dup_every
	
	# NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ë²¡í„°í™”ëœ í™•ì¥ ìˆ˜í–‰
	frames_array = np.array(frames)
	expanded_frames = []
	
	for i, count in enumerate(repeat_counts):
		for _ in range(count):
			expanded_frames.append(frames[i])
	
	return expanded_frames[:target_length]

def expand_face_det_results(face_det_results, target_length):
	"""
	ì–¼êµ´ ê°ì§€ ê²°ê³¼ë¥¼ ëª©í‘œ ê¸¸ì´ë¡œ í™•ì¥
	í”„ë ˆì„ í™•ì¥ê³¼ ë™ì¼í•œ ë¹„ìœ¨ë¡œ ì–¼êµ´ ê°ì§€ ê²°ê³¼ë„ í™•ì¥
	
	Args:
		face_det_results: ì›ë³¸ ì–¼êµ´ ê°ì§€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
		target_length: ëª©í‘œ ê¸¸ì´
		
	Returns:
		í™•ì¥ëœ ì–¼êµ´ ê°ì§€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
	"""
	if len(face_det_results) >= target_length:
		return face_det_results[:target_length]
	
	original_len = len(face_det_results)
	ratio = target_length / original_len
	
	# ê° ê²°ê³¼ì˜ ë°˜ë³µ íšŸìˆ˜ ê³„ì‚°
	repeat_counts = np.ones(original_len, dtype=int)
	remaining = target_length - original_len
	
	if remaining > 0:
		dup_every = original_len / remaining
		next_dup = 0.0
		added = 0
		
		while added < remaining and next_dup < original_len:
			idx = int(next_dup)
			if idx < original_len:
				repeat_counts[idx] += 1
				added += 1
			next_dup += dup_every
	
	# ì–¼êµ´ ê°ì§€ ê²°ê³¼ í™•ì¥ (ì´ë¯¸ì§€ ë³µì‚¬ë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„)
	expanded_results = []
	for i, count in enumerate(repeat_counts):
		for _ in range(count):
			# ì–¼êµ´ ì˜ì—­ê³¼ ì¢Œí‘œë¥¼ ë³µì‚¬ (ì´ë¯¸ì§€ ìì²´ëŠ” ì°¸ì¡°)
			face_img, coords = face_det_results[i]
			expanded_results.append([face_img, coords])
	
	return expanded_results[:target_length]

def get_smoothened_boxes(boxes, T):
	for i in range(len(boxes)):
		if i + T > len(boxes):
			window = boxes[len(boxes) - T:]
		else:
			window = boxes[i : i + T]
		boxes[i] = np.mean(window, axis=0)
	return boxes

# ì „ì—­ detector ìºì‹œ (ìš”ì²­ ê°„ ì¬ì‚¬ìš©)
_detector_cache = {}
_detector_cache_lock = None

def _get_thread_lock():
	"""Thread-safe lock ì´ˆê¸°í™”"""
	global _detector_cache_lock
	if _detector_cache_lock is None:
		import threading
		_detector_cache_lock = threading.Lock()
	return _detector_cache_lock

def face_detect(images, device, face_detector='scrfd', face_det_batch_size=16, pads=[0, 10, 0, 0], nosmooth=False):
	# Detector ìºì‹±: ë™ì¼í•œ deviceì™€ face_detector ì¡°í•©ì€ ì¬ì‚¬ìš©
	cache_key = (device, face_detector)
	
	# Thread-safe ìºì‹œ ì¡°íšŒ
	lock = _get_thread_lock()
	with lock:
		if cache_key not in _detector_cache:
			print(f"[Face Detection] Initializing {face_detector} detector on {device} (first time, will be cached)...")
			_detector_cache[cache_key] = face_detection.FaceAlignment(
				face_detection.LandmarksType._2D, 
				flip_input=False, device=device,
				face_detector=face_detector,
				verbose=True  # ë°°ì¹˜ ì²˜ë¦¬ ë””ë²„ê¹…ì„ ìœ„í•´ ê°•ì œ í™œì„±í™”
			)
			print(f"[Face Detection] âœ… {face_detector} detector initialized and cached")
		else:
			print(f"[Face Detection] ğŸš€ Using cached {face_detector} detector on {device} (fast path)")
		
		detector = _detector_cache[cache_key]

	batch_size = face_det_batch_size
	
	while 1:
		predictions = []
		try:
			for i in tqdm(range(0, len(images), batch_size)):
				batch_end = min(i + batch_size, len(images))
				batch = images[i:batch_end]
				
				# ì‘ì€ ë§ˆì§€ë§‰ ë°°ì¹˜ ìµœì í™”: ë§ˆì§€ë§‰ ë°°ì¹˜ê°€ 8ê°œ ì´í•˜ì´ë©´ ì´ì „ ë°°ì¹˜ì™€ í•©ì¹˜ê¸°
				if len(batch) < batch_size and len(batch) <= 8 and i > 0 and len(predictions) > 0:
					# ì´ì „ ë°°ì¹˜ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ê³¼ í•©ì³ì„œ ì²˜ë¦¬ (ì¤‘ë³µ í—ˆìš©, ë‚˜ì¤‘ì— ì œê±°)
					prev_start = max(0, i - len(batch))
					combined_batch = images[prev_start:batch_end]
					if len(combined_batch) <= batch_size:
						# í•©ì¹œ ë°°ì¹˜ ì²˜ë¦¬
						batch_results = detector.get_detections_for_batch(np.array(combined_batch))
						# ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„ë§Œ predictionsì— ì¶”ê°€
						overlap = i - prev_start
						predictions.extend(batch_results[overlap:])
						break
				
				# ì¼ë°˜ ë°°ì¹˜ ì²˜ë¦¬
				predictions.extend(detector.get_detections_for_batch(np.array(batch)))
		except RuntimeError:
			if batch_size == 1: 
				raise RuntimeError('Image too big to run face detection on GPU. Please use the --resize_factor argument')
			batch_size //= 2
			print('Recovering from OOM error; New batch size: {}'.format(batch_size))
			continue
		break

	results = []
	pady1, pady2, padx1, padx2 = pads
	for rect, image in zip(predictions, images):
		if rect is None:
			cv2.imwrite('temp/faulty_frame.jpg', image)
			raise ValueError('Face not detected! Ensure the video contains a face in all the frames.')

		y1 = int(max(0, rect[1] - pady1))
		y2 = int(min(image.shape[0], rect[3] + pady2))
		x1 = int(max(0, rect[0] - padx1))
		x2 = int(min(image.shape[1], rect[2] + padx2))
		
		results.append([x1, y1, x2, y2])

	boxes = np.array(results)
	if not nosmooth: boxes = get_smoothened_boxes(boxes, T=5)
	results = [[image[y1: y2, x1:x2], (y1, y2, x1, x2)] for image, (x1, y1, x2, y2) in zip(images, boxes)]

	del detector
	return results 

def datagen(frames, mels, static=False, box=[-1, -1, -1, -1], face_det_results=None, img_size=96, batch_size=128):
	img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []

	if box[0] == -1:
		if not static:
			face_det_results = face_det_results
		else:
			face_det_results = face_det_results[:1] if face_det_results else []
	else:
		print('Using the specified bounding box instead of face detection...')
		y1, y2, x1, x2 = box
		face_det_results = [[f[y1: y2, x1:x2], (y1, y2, x1, x2)] for f in frames]

	for i, m in enumerate(mels):
		# static ëª¨ë“œê°€ ì•„ë‹ˆë©´ ì¸ë±ìŠ¤ ì§ì ‘ ì‚¬ìš© (ì´ë¯¸ í™•ì¥ëœ í”„ë ˆì„ì´ë¯€ë¡œ ìˆœí™˜ ë¶ˆí•„ìš”)
		# static ëª¨ë“œë©´ ì²« í”„ë ˆì„ë§Œ ì‚¬ìš©
		if static:
			idx = 0
		else:
			# í”„ë ˆì„ì´ ì´ë¯¸ ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë§ì¶° í™•ì¥ë˜ì—ˆìœ¼ë¯€ë¡œ ì§ì ‘ ì¸ë±ìŠ¤ ì‚¬ìš©
			idx = min(i, len(frames) - 1)
		
		frame_to_save = frames[idx].copy()
		face, coords = face_det_results[idx].copy()

		face = cv2.resize(face, (img_size, img_size))
			
		img_batch.append(face)
		mel_batch.append(m)
		frame_batch.append(frame_to_save)
		coords_batch.append(coords)

		if len(img_batch) >= batch_size:
			img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)

			img_masked = img_batch.copy()
			img_masked[:, img_size//2:] = 0

			img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.
			mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])

			yield img_batch, mel_batch, frame_batch, coords_batch
			img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []

	if len(img_batch) > 0:
		img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)

		img_masked = img_batch.copy()
		img_masked[:, img_size//2:] = 0

		img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.
		mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])

		yield img_batch, mel_batch, frame_batch, coords_batch

def run_wav2lip_inference(
	model,
	face_video_path: str,
	audio_path: str,
	output_path: str,
	device: str = 'cuda',
	wav2lip_batch_size: int = 24,
	face_det_batch_size: int = 12,
	face_detector: str = 'scrfd',
	pads: list = [0, 15, 0, 0],
	resize_factor: int = 1,
	box: list = [-1, -1, -1, -1],
	static: bool = False,
	nosmooth: bool = False,
	video_speed: float = 1.0,
	audio_speed: float = 0.8
):
	"""
	Wav2Lip inference ì‹¤í–‰ (ëª¨ë¸ ì¬ì‚¬ìš©)
	
	Args:
		model: ì´ë¯¸ ë¡œë“œëœ Wav2Lip ëª¨ë¸
		face_video_path: ì–¼êµ´ ì˜ìƒ ê²½ë¡œ
		audio_path: ì˜¤ë””ì˜¤ ê²½ë¡œ
		output_path: ì¶œë ¥ ì˜ìƒ ê²½ë¡œ
		device: 'cuda' or 'cpu'
		wav2lip_batch_size: Wav2Lip ë°°ì¹˜ í¬ê¸°
		face_det_batch_size: ì–¼êµ´ ê°ì§€ ë°°ì¹˜ í¬ê¸°
		face_detector: 'sfd' or 'scrfd'
		pads: [top, bottom, left, right] íŒ¨ë”©
		resize_factor: í•´ìƒë„ ì¶•ì†Œ ë¹„ìœ¨
		box: [y1, y2, x1, x2] ê³ ì • ë°”ìš´ë”© ë°•ìŠ¤
		static: ì •ì  ì´ë¯¸ì§€ ëª¨ë“œ
		nosmooth: ì–¼êµ´ ê°ì§€ ìŠ¤ë¬´ë”© ë¹„í™œì„±í™”
		video_speed: ì˜ìƒ ë°°ì† ì¡°ì ˆ (1.0 = ì •ìƒ, 0.5 = 2ë°° ëŠë¦¬ê²Œ, 2.0 = 2ë°° ë¹ ë¥´ê²Œ)
		audio_speed: ì˜¤ë””ì˜¤ ë°°ì† ì¡°ì ˆ (1.0 = ì •ìƒ, 0.8 = 1.25ë°° ëŠë¦¬ê²Œ, 0.5 = 2ë°° ëŠë¦¬ê²Œ)
	"""
	pipeline_start = time.time()  # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ì‹œê°„
	
	# ============================================
	# 0ë‹¨ê³„: ì˜ìƒ ì½ê¸°
	# ============================================
	step_start = time.time()
	fps = 18.0  # ë¬´ì¡°ê±´ 18fpsë¡œ ê³ ì •
	if os.path.isfile(face_video_path) and face_video_path.split('.')[-1] in ['jpg', 'png', 'jpeg']:
		full_frames = [cv2.imread(face_video_path)]
		static = True
		print(f"[Step 0] Reading image file: {face_video_path}")
	else:
		video_stream = cv2.VideoCapture(face_video_path)
		# ì›ë³¸ FPSëŠ” ì½ê¸°ë§Œ í•˜ê³  ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (18fpsë¡œ ê³ ì •)
		original_fps_read = video_stream.get(cv2.CAP_PROP_FPS)
		print(f"[Step 0] Reading video file: {face_video_path}")
		print(f"  - Original video FPS (not used): {original_fps_read:.2f}, using fixed 18.0 fps")

		full_frames = []
		while 1:
			still_reading, frame = video_stream.read()
			if not still_reading:
				video_stream.release()
				break
			
			if resize_factor > 1:
				frame = cv2.resize(frame, (frame.shape[1]//resize_factor, frame.shape[0]//resize_factor))
			full_frames.append(frame)

	step_time = time.time() - step_start
	print(f"[Step 0] Video reading completed: {len(full_frames)} frames in {step_time:.2f}s")

	# ============================================
	# 1ë‹¨ê³„: ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë° ë°°ì† ì¡°ì •
	# ============================================
	step_start = time.time()
	print(f"[Step 1] Audio processing started")
	
	# 1-1. ì˜¤ë””ì˜¤ ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
	if not audio_path.endswith('.wav'):
		convert_start = time.time()
		print(f"  [1-1] Converting audio to WAV format...")
		temp_wav = 'temp/temp.wav'
		os.makedirs('temp', exist_ok=True)
		command = f'ffmpeg -y -i {audio_path} -strict -2 {temp_wav}'
		subprocess.call(command, shell=platform.system() != 'Windows')
		audio_path = temp_wav
		convert_time = time.time() - convert_start
		print(f"  [1-1] Audio conversion completed in {convert_time:.2f}s")
	else:
		print(f"  [1-1] Audio already in WAV format, skipping conversion")

	# 1-2. ì˜¤ë””ì˜¤ ë°°ì† ì¡°ì • (ëŠë¦¬ê²Œ ë§Œë“¤ê¸°)
	if audio_speed != 1.0:
		speed_start = time.time()
		print(f"  [1-2] Adjusting audio speed to {audio_speed}x (slower)...")
		slowed_audio_path = 'temp/temp_slowed.wav'
		os.makedirs('temp', exist_ok=True)
		# atempo í•„í„° ì‚¬ìš© (0.5 ~ 2.0 ë²”ìœ„, ê·¸ ì´ìƒì€ ì²´ì¸ í•„ìš”)
		if audio_speed < 0.5:
			# 0.5ë³´ë‹¤ ì‘ìœ¼ë©´ ì—¬ëŸ¬ ë²ˆ ì²´ì¸
			atempo_value = 0.5
			chain_count = int(np.ceil(np.log(audio_speed) / np.log(0.5)))
			atempo_filter = ','.join(['atempo=0.5'] * chain_count)
		elif audio_speed > 2.0:
			# 2.0ë³´ë‹¤ í¬ë©´ ì—¬ëŸ¬ ë²ˆ ì²´ì¸
			atempo_value = 2.0
			chain_count = int(np.ceil(np.log(audio_speed) / np.log(2.0)))
			atempo_filter = ','.join(['atempo=2.0'] * chain_count)
		else:
			atempo_filter = f'atempo={audio_speed}'
		
		command = f'ffmpeg -y -i {audio_path} -af "{atempo_filter}" -strict -2 {slowed_audio_path}'
		subprocess.call(command, shell=platform.system() != 'Windows')
		audio_path = slowed_audio_path
		speed_time = time.time() - speed_start
		print(f"  [1-2] Audio speed adjustment completed in {speed_time:.2f}s")
	else:
		print(f"  [1-2] Audio speed adjustment skipped (audio_speed=1.0)")

	# 1-3. Mel spectrogram ìƒì„±
	mel_start = time.time()
	print(f"  [1-3] Generating mel spectrogram...")
	wav = audio.load_wav(audio_path, 16000)
	mel = audio.melspectrogram(wav)
	mel_time = time.time() - mel_start
	print(f"  [1-3] Mel spectrogram generated: shape {mel.shape} in {mel_time:.2f}s")

	if np.isnan(mel.reshape(-1)).sum() > 0:
		raise ValueError('Mel contains nan! Using a TTS voice? Add a small epsilon noise to the wav file and try again')

	# 1-4. Mel chunks ìƒì„± (ëŠë ¤ì§„ ì˜¤ë””ì˜¤ ê¸¸ì´ ê¸°ì¤€)
	chunks_start = time.time()
	print(f"  [1-4] Creating mel chunks...")
	mel_chunks = []
	mel_idx_multiplier = 80./fps  # ì›ë³¸ FPS ê¸°ì¤€
	i = 0
	while 1:
		start_idx = int(i * mel_idx_multiplier)
		if start_idx + mel_step_size > len(mel[0]):
			mel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])
			break
		mel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])
		i += 1
	chunks_time = time.time() - chunks_start
	target_frame_count = len(mel_chunks)
	
	step_time = time.time() - step_start
	print(f"[Step 1] Audio processing completed in {step_time:.2f}s")
	print(f"  - Audio length: {target_frame_count} frames (mel chunks)")
	print(f"  - Original video frames: {len(full_frames)}")

	# ============================================
	# 2ë‹¨ê³„: ë°°ì† ì¡°ì •ëœ ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë§ì¶° ì˜ìƒ ê¸¸ì´ ì¡°ì •
	# ============================================
	step_start = time.time()
	print(f"[Step 2] Video frame adjustment started")
	original_frames = full_frames.copy()
	# ì›ë³¸ FPSëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ (24fps ë“±)
	
	if len(full_frames) < target_frame_count:
		extend_start = time.time()
		print(f"  [2-1] Extending video frames from {len(full_frames)} to {target_frame_count} to match slowed audio length...")
		full_frames = increase_frames(full_frames, target_frame_count)
		extend_time = time.time() - extend_start
		# FPSëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ ìœ ì§€ (24fps ë“±) - ë¹ ë¥´ê²Œ ì¬ìƒë˜ì§€ ì•Šë„ë¡
		print(f"  [2-1] Frame extension completed in {extend_time:.2f}s")
		print(f"  - Maintaining original FPS: {fps:.2f} (frames extended but FPS unchanged)")
	elif len(full_frames) > target_frame_count:
		trim_start = time.time()
		print(f"  [2-1] Trimming video frames from {len(full_frames)} to {target_frame_count} to match audio length...")
		full_frames = full_frames[:target_frame_count]
		trim_time = time.time() - trim_start
		print(f"  [2-1] Frame trimming completed in {trim_time:.2f}s")
		# FPSëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ (í”„ë ˆì„ ìˆ˜ë§Œ ì¤„ì„)
	else:
		print(f"  [2-1] Video and audio lengths match perfectly, no adjustment needed")
	
	step_time = time.time() - step_start
	print(f"[Step 2] Video frame adjustment completed in {step_time:.2f}s")
	print(f"  - Final frame count: {len(full_frames)}")

	# ============================================
	# 3ë‹¨ê³„: ì¡°ì •ëœ ì˜ìƒì—ì„œ ì–¼êµ´ íƒì§€ (ìµœì í™”)
	# ============================================
	step_start = time.time()
	print(f"[Step 3] Face detection started")
	if box[0] == -1:
		if not static:
			# ìµœì í™”: ì›ë³¸ í”„ë ˆì„ë§Œ ì–¼êµ´ ê°ì§€ í›„ ê²°ê³¼ í™•ì¥
			if len(original_frames) < target_frame_count:
				# ì˜ìƒì´ í™•ì¥ëœ ê²½ìš°: ì›ë³¸ë§Œ ê°ì§€ í›„ ê²°ê³¼ í™•ì¥
				print(f"  [3-1] Optimized face detection: detecting {len(original_frames)} original frames, then expanding results...")
				face_det_start = time.time()
				face_det_results_original = face_detect(original_frames, device, face_detector, face_det_batch_size, pads, nosmooth)
				face_det_time = time.time() - face_det_start
				print(f"  [3-1] Face detection completed in {face_det_time:.2f}s")
				
				expand_start = time.time()
				print(f"  [3-2] Expanding face detection results from {len(original_frames)} to {target_frame_count}...")
				face_det_results = expand_face_det_results(face_det_results_original, target_frame_count)
				expand_time = time.time() - expand_start
				print(f"  [3-2] Result expansion completed in {expand_time:.2f}s")
			else:
				# ì˜ìƒì´ ì˜ë¦° ê²½ìš°: ì˜ë¦° í”„ë ˆì„ì— ëŒ€í•´ ê°ì§€
				print(f"  [3-1] Face detection on {len(full_frames)} frames...")
				face_det_start = time.time()
				face_det_results = face_detect(full_frames, device, face_detector, face_det_batch_size, pads, nosmooth)
				face_det_time = time.time() - face_det_start
				print(f"  [3-1] Face detection completed in {face_det_time:.2f}s")
		else:
			print(f"  [3-1] Static mode: detecting face on first frame only...")
			face_det_start = time.time()
			face_det_results = face_detect([full_frames[0]], device, face_detector, face_det_batch_size, pads, nosmooth)
			face_det_time = time.time() - face_det_start
			print(f"  [3-1] Face detection completed in {face_det_time:.2f}s")
			# static ëª¨ë“œì—ì„œë„ í™•ì¥ í•„ìš”
			if target_frame_count > 1:
				expand_start = time.time()
				print(f"  [3-2] Expanding face detection results from 1 to {target_frame_count}...")
				face_det_results = expand_face_det_results(face_det_results, target_frame_count)
				expand_time = time.time() - expand_start
				print(f"  [3-2] Result expansion completed in {expand_time:.2f}s")
	else:
		print('  [3-1] Using the specified bounding box instead of face detection...')
		bbox_start = time.time()
		y1, y2, x1, x2 = box
		face_det_results = [[f[y1: y2, x1:x2], (y1, y2, x1, x2)] for f in full_frames]
		bbox_time = time.time() - bbox_start
		print(f"  [3-1] Bounding box extraction completed in {bbox_time:.2f}s")
	
	step_time = time.time() - step_start
	print(f"[Step 3] Face detection completed in {step_time:.2f}s")
	print(f"  - Total face detection results: {len(face_det_results)}")

	# ============================================
	# 4ë‹¨ê³„: Inference ì‹¤í–‰
	# ============================================
	step_start = time.time()
	print(f"[Step 4] Wav2Lip inference started")
	batch_size = wav2lip_batch_size
	gen = datagen(full_frames.copy(), mel_chunks, static, box, face_det_results, img_size=96, batch_size=batch_size)
	
	frame_h, frame_w = full_frames[0].shape[:-1]
	os.makedirs('temp', exist_ok=True)
	
	# FPS í•˜ë“œì½”ë”©: ë¬´ì¡°ê±´ 18fps
	fps = 18.0
	print(f"  - Video output settings: {frame_w}x{frame_h} @ {fps:.2f}fps")
	out = cv2.VideoWriter('temp/result.avi', 
							cv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_w, frame_h))

	inference_start = time.time()
	postprocess_start = None
	postprocess_time = 0
	
	for i, (img_batch, mel_batch, frames, coords) in enumerate(tqdm(gen, 
										total=int(np.ceil(float(len(mel_chunks))/batch_size)))):
		if i == 0:
			batch_start = time.time()
		
		# 4-1. ì…ë ¥ ë³€í™˜
		if i == 0:
			convert_start = time.time()
		if next(model.parameters()).dtype == torch.float16:
			img_batch = torch.HalfTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)
			mel_batch = torch.HalfTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)
		else:
			img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)
			mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)
		if i == 0:
			convert_time = time.time() - convert_start
			print(f"  [4-1] First batch input conversion: {convert_time:.3f}s")

		# 4-2. ëª¨ë¸ Inference
		if i == 0:
			model_start = time.time()
		with torch.no_grad():
			if device == 'cuda' and next(model.parameters()).dtype == torch.float16:
				with torch.amp.autocast('cuda'):
					pred = model(mel_batch, img_batch)
			else:
				pred = model(mel_batch, img_batch)
		if i == 0:
			model_time = time.time() - model_start
			print(f"  [4-2] First batch model inference: {model_time:.3f}s")

		# 4-3. í›„ì²˜ë¦¬ ì‹œì‘ ì‹œê°„ ì¸¡ì •
		if postprocess_start is None:
			postprocess_start = time.time()
		
		# CUDA í…ì„œë¥¼ CPUë¡œ ì´ë™ í›„ numpyë¡œ ë³€í™˜
		pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.
		
		for p, f, c in zip(pred, frames, coords):
			y1, y2, x1, x2 = c
			y1, y2, x1, x2 = int(y1), int(y2), int(x1), int(x2)
			
			target_width = x2 - x1
			target_height = y2 - y1
			p = cv2.resize(p.astype(np.uint8), (target_width, target_height))
			
			# í˜ë”ë§
			feather_amount = min(15, max(5, target_width // 15, target_height // 15))
			
			if feather_amount > 0:
				mask = np.ones((target_height, target_width), dtype=np.float32)
				fade_range = np.arange(feather_amount, dtype=np.float32) / feather_amount
				
				mask[:feather_amount, :] *= fade_range[:, np.newaxis]
				mask[-feather_amount:, :] *= fade_range[::-1, np.newaxis]
				mask[:, :feather_amount] *= fade_range[np.newaxis, :]
				mask[:, -feather_amount:] *= fade_range[::-1, np.newaxis].T
				
				mask = mask[:, :, np.newaxis]
				original_region = f[y1:y2, x1:x2].astype(np.float32)
				blended = (p.astype(np.float32) * mask + original_region * (1 - mask)).astype(np.uint8)
				
				f[y1:y2, x1:x2] = blended
			else:
				f[y1:y2, x1:x2] = p
			
			out.write(f)
	
	postprocess_time = time.time() - postprocess_start if postprocess_start else 0
	inference_time = time.time() - inference_start
	out.release()
	
	step_time = time.time() - step_start
	print(f"[Step 4] Wav2Lip inference completed in {step_time:.2f}s")
	print(f"  - Total inference time: {inference_time:.2f}s")
	print(f"  - Post-processing time: {postprocess_time:.2f}s")
	print(f"  - Processed {len(mel_chunks)} frames in {int(np.ceil(float(len(mel_chunks))/batch_size))} batches")

	# ============================================
	# 5ë‹¨ê³„: ì˜¤ë””ì˜¤ í•©ì„±
	# ============================================
	step_start = time.time()
	print(f"[Step 5] Audio-video synthesis started")
	command = f'ffmpeg -y -i {audio_path} -i temp/result.avi -strict -2 -q:v 1 {output_path}'
	subprocess.call(command, shell=platform.system() != 'Windows')
	step_time = time.time() - step_start
	print(f"[Step 5] Audio-video synthesis completed in {step_time:.2f}s")
	
	total_time = time.time() - pipeline_start
	print(f"\n[Summary] Output saved to: {output_path}")
	print(f"  - Total pipeline time: {total_time:.2f}s")

